{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Social Capital Analytics Challenge\n",
        "\n",
        "---\n",
        "\n",
        "The Social Capital\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RmVUvfZptN69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload and Clean Data"
      ],
      "metadata": {
        "id": "Iw-wM82jA_86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import linear_model\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import statsmodels.api as sm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "!pip install statsmodels\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from statsmodels.imputation import mice\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaJwrsIpZBag",
        "outputId": "364336a3-79a0-4f7a-bb85-7faf1c1bc2bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (0.14.1)\n",
            "Requirement already satisfied: numpy<2,>=1.18 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.23.5)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.11.4)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.5.3)\n",
            "Requirement already satisfied: patsy>=0.5.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (23.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2023.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.4->statsmodels) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Z6S1yEZCusqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "248349e2-ac9a-4416-ca36-526b49168d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data (joined w/ county pop. stats to address nulls)\n",
        "sc = pd.read_csv(\"/content/drive/MyDrive/Social Capital Data/merged_df.csv\")\n",
        "# Drop columns not needed or wrong data type\n",
        "sc = sc.drop(columns=['Unnamed: 0', 'county', 'county_name', 'child_ec_se_county', 'ec_high_se_county', 'child_high_ec_se_county', 'ec_se_county' ])"
      ],
      "metadata": {
        "id": "3H_Qv3NizBom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To address missingness of 106 counties population statistics, we sourced cencus data from 2021 and used it to analagously represent population for the year 2018 to best integrate with existing data. Then, we utilized a complex Excel formula to join the missing population statistics for each county, **resulting in null values for 'pop2018' to go from 106 to 0**, with a high level of confidence in our manual imputation.\n",
        "\n",
        "\n",
        "\n",
        "=IF(ISBLANK(C2), \"Missing\", IF(ISERROR(VLOOKUP(C2, A:A, 1, FALSE)), \"Missing\", VLOOKUP(C2, A:B, 2, FALSE)))\n",
        "\n",
        "\n",
        "\n",
        "Source: https://www.census.gov/data/tables/time-series/demo/popest/2020s-counties-total.html"
      ],
      "metadata": {
        "id": "Ck8UdsVu-9tS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3bC9C9d3Ihb",
        "outputId": "ed6d3c46-ca0b-4db3-9713-d46b6d9024db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "num_below_p50                   float64\n",
              "pop2018                         float64\n",
              "ec_county                       float64\n",
              "child_ec_county                 float64\n",
              "ec_grp_mem_county               float64\n",
              "ec_high_county                  float64\n",
              "child_high_ec_county            float64\n",
              "ec_grp_mem_high_county          float64\n",
              "exposure_grp_mem_county         float64\n",
              "exposure_grp_mem_high_county    float64\n",
              "child_exposure_county           float64\n",
              "child_high_exposure_county      float64\n",
              "bias_grp_mem_county             float64\n",
              "bias_grp_mem_high_county        float64\n",
              "child_bias_county               float64\n",
              "child_high_bias_county          float64\n",
              "clustering_county               float64\n",
              "support_ratio_county            float64\n",
              "volunteering_rate_county        float64\n",
              "civic_organizations_county      float64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTu67o2l6-B-",
        "outputId": "b86c9981-9a2a-41e0-ab80-47e7630a68c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "num_below_p50                     2\n",
              "pop2018                           0\n",
              "ec_county                        71\n",
              "child_ec_county                 360\n",
              "ec_grp_mem_county                77\n",
              "ec_high_county                   71\n",
              "child_high_ec_county            360\n",
              "ec_grp_mem_high_county           77\n",
              "exposure_grp_mem_county          77\n",
              "exposure_grp_mem_high_county     77\n",
              "child_exposure_county           360\n",
              "child_high_exposure_county      360\n",
              "bias_grp_mem_county              77\n",
              "bias_grp_mem_high_county         77\n",
              "child_bias_county               360\n",
              "child_high_bias_county          360\n",
              "clustering_county                 0\n",
              "support_ratio_county              0\n",
              "volunteering_rate_county          0\n",
              "civic_organizations_county        0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imputation of child metrics"
      ],
      "metadata": {
        "id": "FSBZeMvLUVlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# child_ec_county imputation\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class NumericImputer:\n",
        "    def __init__(self, n_neighbors=5):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.knn_regressor = KNeighborsRegressor(n_neighbors=self.n_neighbors)\n",
        "\n",
        "    def train_model(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Ensure feature_variables is a list for consistent handling\n",
        "        if not isinstance(feature_variables, list):\n",
        "            feature_variables = [feature_variables]  # Make it a list if it's not\n",
        "\n",
        "        # Filter the dataframe to create a training set\n",
        "        training_data = data.dropna(subset=feature_variables + [target_variable])\n",
        "        training_data = training_data.loc[\n",
        "            (training_data[target_variable].notnull()) &\n",
        "            (training_data[feature_variables[0]] <= max_pop_threshold),\n",
        "            feature_variables + [target_variable]\n",
        "        ]\n",
        "\n",
        "        X_train = training_data[feature_variables]\n",
        "        y_train = training_data[target_variable]\n",
        "\n",
        "        # Fit the KNN model on the training data\n",
        "        self.knn_regressor.fit(X_train, y_train)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def impute_missing_values(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Identify the subset with missing target variable values\n",
        "        missing_data_subset = data.loc[\n",
        "            data[target_variable].isnull() & (data[feature_variables[0]] <= max_pop_threshold),\n",
        "            feature_variables\n",
        "        ]\n",
        "\n",
        "        # Predict the target variable for the missing data subset\n",
        "        predicted_values = self.knn_regressor.predict(missing_data_subset)\n",
        "\n",
        "        # Impute the predicted values back into the original dataframe\n",
        "        data.loc[missing_data_subset.index, target_variable] = predicted_values\n",
        "\n",
        "        return data\n",
        "\n",
        "    def evaluate_model(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Prepare the dataset for cross-validation\n",
        "        valid_data = data.loc[data[target_variable].notnull() & (data[feature_variables[0]] <= max_pop_threshold)]\n",
        "        X = valid_data[feature_variables]\n",
        "        y = valid_data[target_variable]\n",
        "\n",
        "        # Define a custom scorer function\n",
        "        def mae_scorer(estimator, X, y):\n",
        "            predicted_values = estimator.predict(X)\n",
        "            return -np.mean(np.abs(predicted_values - y))  # Negative MAE\n",
        "\n",
        "        # Perform 5-Fold cross-validation to estimate the MAE\n",
        "        cv_scores = cross_val_score(self.knn_regressor, X, y, cv=5, scoring=mae_scorer)\n",
        "\n",
        "        # Return the average MAE\n",
        "        return -cv_scores.mean()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == '__main__':\n",
        "    data = pd.DataFrame(sc)  # Your DataFrame here\n",
        "    target_variable = 'child_ec_county'  # The variable you wish to impute\n",
        "    feature_variables = ['support_ratio_county']\n",
        "    max_pop_threshold = 100000  # Example threshold value\n",
        "\n",
        "    imputer = NumericImputer(n_neighbors=5)\n",
        "    imputer.train_model(data, target_variable, feature_variables, max_pop_threshold)\n",
        "    imputed_data = imputer.impute_missing_values(data, target_variable, feature_variables, max_pop_threshold)\n",
        "    mae_estimate = imputer.evaluate_model(data, target_variable, feature_variables, max_pop_threshold)\n",
        "\n",
        "    print('MAE Estimate:', mae_estimate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4tQmxLBzenm",
        "outputId": "53dc6592-010b-4823-df34-0d86c9719ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE Estimate: 0.1462141139530802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# child_high_ec_county imputation\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class NumericImputer:\n",
        "    def __init__(self, n_neighbors=5):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.knn_regressor = KNeighborsRegressor(n_neighbors=self.n_neighbors)\n",
        "\n",
        "    def train_model(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Ensure feature_variables is a list for consistent handling\n",
        "        if not isinstance(feature_variables, list):\n",
        "            feature_variables = [feature_variables]  # Make it a list if it's not\n",
        "\n",
        "        # Filter the dataframe to create a training set\n",
        "        training_data = data.dropna(subset=feature_variables + [target_variable])\n",
        "        training_data = training_data.loc[\n",
        "            (training_data[target_variable].notnull()) &\n",
        "            (training_data[feature_variables[0]] <= max_pop_threshold),\n",
        "            feature_variables + [target_variable]\n",
        "        ]\n",
        "\n",
        "        X_train = training_data[feature_variables]\n",
        "        y_train = training_data[target_variable]\n",
        "\n",
        "        # Fit the KNN model on the training data\n",
        "        self.knn_regressor.fit(X_train, y_train)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def impute_missing_values(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Identify the subset with missing target variable values\n",
        "        missing_data_subset = data.loc[\n",
        "            data[target_variable].isnull() & (data[feature_variables[0]] <= max_pop_threshold),\n",
        "            feature_variables\n",
        "        ]\n",
        "\n",
        "        # Predict the target variable for the missing data subset\n",
        "        predicted_values = self.knn_regressor.predict(missing_data_subset)\n",
        "\n",
        "        # Impute the predicted values back into the original dataframe\n",
        "        data.loc[missing_data_subset.index, target_variable] = predicted_values\n",
        "\n",
        "        return data\n",
        "\n",
        "    def evaluate_model(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Prepare the dataset for cross-validation\n",
        "        valid_data = data.loc[data[target_variable].notnull() & (data[feature_variables[0]] <= max_pop_threshold)]\n",
        "        X = valid_data[feature_variables]\n",
        "        y = valid_data[target_variable]\n",
        "\n",
        "        # Define a custom scorer function\n",
        "        def mae_scorer(estimator, X, y):\n",
        "            predicted_values = estimator.predict(X)\n",
        "            return -np.mean(np.abs(predicted_values - y))  # Negative MAE\n",
        "\n",
        "        # Perform 5-Fold cross-validation to estimate the MAE\n",
        "        cv_scores = cross_val_score(self.knn_regressor, X, y, cv=5, scoring=mae_scorer)\n",
        "\n",
        "        # Return the average MAE\n",
        "        return -cv_scores.mean()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == '__main__':\n",
        "    data = pd.DataFrame(sc)  # Your DataFrame here\n",
        "    target_variable = 'child_high_ec_county'  # The variable you wish to impute\n",
        "    feature_variables = ['support_ratio_county']\n",
        "    max_pop_threshold = 100000  # Example threshold value\n",
        "\n",
        "    imputer = NumericImputer(n_neighbors=5)\n",
        "    imputer.train_model(data, target_variable, feature_variables, max_pop_threshold)\n",
        "    imputed_data = imputer.impute_missing_values(data, target_variable, feature_variables, max_pop_threshold)\n",
        "    mae_estimate = imputer.evaluate_model(data, target_variable, feature_variables, max_pop_threshold)\n",
        "\n",
        "    print('MAE Estimate:', mae_estimate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUXn5G_-DtrI",
        "outputId": "5dcf17f1-c810-4b08-e5c1-666a68cc874e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE Estimate: 0.15864721767449622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# child_exposure_county\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class NumericImputer:\n",
        "    def __init__(self, n_neighbors=5):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.knn_regressor = KNeighborsRegressor(n_neighbors=self.n_neighbors)\n",
        "\n",
        "    def train_model(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Ensure feature_variables is a list for consistent handling\n",
        "        if not isinstance(feature_variables, list):\n",
        "            feature_variables = [feature_variables]  # Make it a list if it's not\n",
        "\n",
        "        # Filter the dataframe to create a training set\n",
        "        training_data = data.dropna(subset=feature_variables + [target_variable])\n",
        "        training_data = training_data.loc[\n",
        "            (training_data[target_variable].notnull()) &\n",
        "            (training_data[feature_variables[0]] <= max_pop_threshold),\n",
        "            feature_variables + [target_variable]\n",
        "        ]\n",
        "\n",
        "        X_train = training_data[feature_variables]\n",
        "        y_train = training_data[target_variable]\n",
        "\n",
        "        # Fit the KNN model on the training data\n",
        "        self.knn_regressor.fit(X_train, y_train)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def impute_missing_values(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Identify the subset with missing target variable values\n",
        "        missing_data_subset = data.loc[\n",
        "            data[target_variable].isnull() & (data[feature_variables[0]] <= max_pop_threshold),\n",
        "            feature_variables\n",
        "        ]\n",
        "\n",
        "        # Predict the target variable for the missing data subset\n",
        "        predicted_values = self.knn_regressor.predict(missing_data_subset)\n",
        "\n",
        "        # Impute the predicted values back into the original dataframe\n",
        "        data.loc[missing_data_subset.index, target_variable] = predicted_values\n",
        "\n",
        "        return data\n",
        "\n",
        "    def evaluate_model(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Prepare the dataset for cross-validation\n",
        "        valid_data = data.loc[data[target_variable].notnull() & (data[feature_variables[0]] <= max_pop_threshold)]\n",
        "        X = valid_data[feature_variables]\n",
        "        y = valid_data[target_variable]\n",
        "\n",
        "        # Define a custom scorer function\n",
        "        def mae_scorer(estimator, X, y):\n",
        "            predicted_values = estimator.predict(X)\n",
        "            return -np.mean(np.abs(predicted_values - y))  # Negative MAE\n",
        "\n",
        "        # Perform 5-Fold cross-validation to estimate the MAE\n",
        "        cv_scores = cross_val_score(self.knn_regressor, X, y, cv=5, scoring=mae_scorer)\n",
        "\n",
        "        # Return the average MAE\n",
        "        return -cv_scores.mean()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == '__main__':\n",
        "    data = pd.DataFrame(sc)  # Your DataFrame here\n",
        "    target_variable = 'child_exposure_county'  # The variable you wish to impute\n",
        "    feature_variables = ['support_ratio_county']\n",
        "    max_pop_threshold = 100000  # Example threshold value\n",
        "\n",
        "    imputer = NumericImputer(n_neighbors=5)\n",
        "    imputer.train_model(data, target_variable, feature_variables, max_pop_threshold)\n",
        "    imputed_data = imputer.impute_missing_values(data, target_variable, feature_variables, max_pop_threshold)\n",
        "    mae_estimate = imputer.evaluate_model(data, target_variable, feature_variables, max_pop_threshold)\n",
        "\n",
        "    print('MAE Estimate:', mae_estimate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szBgCgSRDugc",
        "outputId": "d4975d60-bf42-4962-a6ab-20ffcbe62fcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE Estimate: 0.1477273992004301\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# child_high_exposure_county\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class NumericImputer:\n",
        "    def __init__(self, n_neighbors=5):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.knn_regressor = KNeighborsRegressor(n_neighbors=self.n_neighbors)\n",
        "\n",
        "    def train_model(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Ensure feature_variables is a list for consistent handling\n",
        "        if not isinstance(feature_variables, list):\n",
        "            feature_variables = [feature_variables]  # Make it a list if it's not\n",
        "\n",
        "        # Filter the dataframe to create a training set\n",
        "        training_data = data.dropna(subset=feature_variables + [target_variable])\n",
        "        training_data = training_data.loc[\n",
        "            (training_data[target_variable].notnull()) &\n",
        "            (training_data[feature_variables[0]] <= max_pop_threshold),\n",
        "            feature_variables + [target_variable]\n",
        "        ]\n",
        "\n",
        "        X_train = training_data[feature_variables]\n",
        "        y_train = training_data[target_variable]\n",
        "\n",
        "        # Fit the KNN model on the training data\n",
        "        self.knn_regressor.fit(X_train, y_train)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def impute_missing_values(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Identify the subset with missing target variable values\n",
        "        missing_data_subset = data.loc[\n",
        "            data[target_variable].isnull() & (data[feature_variables[0]] <= max_pop_threshold),\n",
        "            feature_variables\n",
        "        ]\n",
        "\n",
        "        # Predict the target variable for the missing data subset\n",
        "        predicted_values = self.knn_regressor.predict(missing_data_subset)\n",
        "\n",
        "        # Impute the predicted values back into the original dataframe\n",
        "        data.loc[missing_data_subset.index, target_variable] = predicted_values\n",
        "\n",
        "        return data\n",
        "\n",
        "    def evaluate_model(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Prepare the dataset for cross-validation\n",
        "        valid_data = data.loc[data[target_variable].notnull() & (data[feature_variables[0]] <= max_pop_threshold)]\n",
        "        X = valid_data[feature_variables]\n",
        "        y = valid_data[target_variable]\n",
        "\n",
        "        # Define a custom scorer function\n",
        "        def mae_scorer(estimator, X, y):\n",
        "            predicted_values = estimator.predict(X)\n",
        "            return -np.mean(np.abs(predicted_values - y))  # Negative MAE\n",
        "\n",
        "        # Perform 5-Fold cross-validation to estimate the MAE\n",
        "        cv_scores = cross_val_score(self.knn_regressor, X, y, cv=5, scoring=mae_scorer)\n",
        "\n",
        "        # Return the average MAE\n",
        "        return -cv_scores.mean()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == '__main__':\n",
        "    data = pd.DataFrame(sc)  # Your DataFrame here\n",
        "    target_variable = 'child_high_exposure_county'  # The variable you wish to impute\n",
        "    feature_variables = ['support_ratio_county']\n",
        "    max_pop_threshold = 100000  # Example threshold value\n",
        "\n",
        "    imputer = NumericImputer(n_neighbors=5)\n",
        "    imputer.train_model(data, target_variable, feature_variables, max_pop_threshold)\n",
        "    imputed_data = imputer.impute_missing_values(data, target_variable, feature_variables, max_pop_threshold)\n",
        "    mae_estimate = imputer.evaluate_model(data, target_variable, feature_variables, max_pop_threshold)\n",
        "\n",
        "    print('MAE Estimate:', mae_estimate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5067q0fDvdI",
        "outputId": "60d65a86-dfd3-443f-a947-93688bc37ff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE Estimate: 0.15377747045777093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# child_bias_count_imputation\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class NumericImputer:\n",
        "    def __init__(self, n_neighbors=5):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.knn_regressor = KNeighborsRegressor(n_neighbors=self.n_neighbors)\n",
        "\n",
        "    def train_model(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Ensure feature_variables is a list for consistent handling\n",
        "        if not isinstance(feature_variables, list):\n",
        "            feature_variables = [feature_variables]  # Make it a list if it's not\n",
        "\n",
        "        # Filter the dataframe to create a training set\n",
        "        training_data = data.dropna(subset=feature_variables + [target_variable])\n",
        "        training_data = training_data.loc[\n",
        "            (training_data[target_variable].notnull()) &\n",
        "            (training_data[feature_variables[0]] <= max_pop_threshold),\n",
        "            feature_variables + [target_variable]\n",
        "        ]\n",
        "\n",
        "        X_train = training_data[feature_variables]\n",
        "        y_train = training_data[target_variable]\n",
        "\n",
        "        # Fit the KNN model on the training data\n",
        "        self.knn_regressor.fit(X_train, y_train)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def impute_missing_values(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Identify the subset with missing target variable values\n",
        "        missing_data_subset = data.loc[\n",
        "            data[target_variable].isnull() & (data[feature_variables[0]] <= max_pop_threshold),\n",
        "            feature_variables\n",
        "        ]\n",
        "\n",
        "        # Predict the target variable for the missing data subset\n",
        "        predicted_values = self.knn_regressor.predict(missing_data_subset)\n",
        "\n",
        "        # Impute the predicted values back into the original dataframe\n",
        "        data.loc[missing_data_subset.index, target_variable] = predicted_values\n",
        "\n",
        "        return data\n",
        "\n",
        "    def evaluate_model(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Prepare the dataset for cross-validation\n",
        "        valid_data = data.loc[data[target_variable].notnull() & (data[feature_variables[0]] <= max_pop_threshold)]\n",
        "        X = valid_data[feature_variables]\n",
        "        y = valid_data[target_variable]\n",
        "\n",
        "        # Define a custom scorer function\n",
        "        def mae_scorer(estimator, X, y):\n",
        "            predicted_values = estimator.predict(X)\n",
        "            return -np.mean(np.abs(predicted_values - y))  # Negative MAE\n",
        "\n",
        "        # Perform 5-Fold cross-validation to estimate the MAE\n",
        "        cv_scores = cross_val_score(self.knn_regressor, X, y, cv=5, scoring=mae_scorer)\n",
        "\n",
        "        # Return the average MAE\n",
        "        return -cv_scores.mean()\n",
        "\n",
        "# MAE between child_bias_county and support_ration_county\n",
        "if __name__ == '__main__':\n",
        "    data = pd.DataFrame(sc)\n",
        "    target_variable = 'child_bias_county'\n",
        "    feature_variables = ['support_ratio_county']\n",
        "    max_pop_threshold = 100000\n",
        "\n",
        "    imputer = NumericImputer(n_neighbors=5)\n",
        "    imputer.train_model(data, target_variable, feature_variables, max_pop_threshold)\n",
        "    imputed_data = imputer.impute_missing_values(data, target_variable, feature_variables, max_pop_threshold)\n",
        "    mae_estimate = imputer.evaluate_model(data, target_variable, feature_variables, max_pop_threshold)\n",
        "\n",
        "    print('MAE Estimate:', mae_estimate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCorCTtCDwio",
        "outputId": "ca38ccb0-167c-4878-b1d7-4bbe6f4317b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE Estimate: 0.024329562354170414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# child_high_bias_county imputation\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class NumericImputer:\n",
        "    def __init__(self, n_neighbors=5):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.knn_regressor = KNeighborsRegressor(n_neighbors=self.n_neighbors)\n",
        "\n",
        "    def train_model(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Ensure feature_variables is a list for consistent handling\n",
        "        if not isinstance(feature_variables, list):\n",
        "            feature_variables = [feature_variables]  # Make it a list if it's not\n",
        "\n",
        "        # Filter the dataframe to create a training set\n",
        "        training_data = data.dropna(subset=feature_variables + [target_variable])\n",
        "        training_data = training_data.loc[\n",
        "            (training_data[target_variable].notnull()) &\n",
        "            (training_data[feature_variables[0]] <= max_pop_threshold),\n",
        "            feature_variables + [target_variable]\n",
        "        ]\n",
        "\n",
        "        X_train = training_data[feature_variables]\n",
        "        y_train = training_data[target_variable]\n",
        "\n",
        "        # Fit the KNN model on the training data\n",
        "        self.knn_regressor.fit(X_train, y_train)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def impute_missing_values(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Identify the subset with missing target variable values\n",
        "        missing_data_subset = data.loc[\n",
        "            data[target_variable].isnull() & (data[feature_variables[0]] <= max_pop_threshold),\n",
        "            feature_variables\n",
        "        ]\n",
        "\n",
        "        # Predict the target variable for the missing data subset\n",
        "        predicted_values = self.knn_regressor.predict(missing_data_subset)\n",
        "\n",
        "        # Impute the predicted values back into the original dataframe\n",
        "        data.loc[missing_data_subset.index, target_variable] = predicted_values\n",
        "\n",
        "        return data\n",
        "\n",
        "    def evaluate_model(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Prepare the dataset for cross-validation\n",
        "        valid_data = data.loc[data[target_variable].notnull() & (data[feature_variables[0]] <= max_pop_threshold)]\n",
        "        X = valid_data[feature_variables]\n",
        "        y = valid_data[target_variable]\n",
        "\n",
        "        # Define a custom scorer function\n",
        "        def mae_scorer(estimator, X, y):\n",
        "            predicted_values = estimator.predict(X)\n",
        "            return -np.mean(np.abs(predicted_values - y))  # Negative MAE\n",
        "\n",
        "        # Perform 5-Fold cross-validation to estimate the MAE\n",
        "        cv_scores = cross_val_score(self.knn_regressor, X, y, cv=5, scoring=mae_scorer)\n",
        "\n",
        "        # Return the average MAE\n",
        "        return -cv_scores.mean()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == '__main__':\n",
        "    data = pd.DataFrame(sc)  # Your DataFrame here\n",
        "    target_variable = 'child_high_bias_county'  # The variable you wish to impute\n",
        "    feature_variables = ['support_ratio_county']\n",
        "    max_pop_threshold = 100000  # Example threshold value\n",
        "\n",
        "    imputer = NumericImputer(n_neighbors=5)\n",
        "    imputer.train_model(data, target_variable, feature_variables, max_pop_threshold)\n",
        "    imputed_data = imputer.impute_missing_values(data, target_variable, feature_variables, max_pop_threshold)\n",
        "    mae_estimate = imputer.evaluate_model(data, target_variable, feature_variables, max_pop_threshold)\n",
        "\n",
        "    print('MAE Estimate:', mae_estimate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1gwNggXGIGj",
        "outputId": "c3eb34bf-cc17-4164-fac8-d69116556476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE Estimate: 0.03581092584877005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imputation of ec metrics"
      ],
      "metadata": {
        "id": "7dKSA9K5UsOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ec_county imputation\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class NumericImputer:\n",
        "    def __init__(self, n_neighbors=5):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.knn_regressor = KNeighborsRegressor(n_neighbors=self.n_neighbors)\n",
        "\n",
        "    def train_model(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Ensure feature_variables is a list for consistent handling\n",
        "        if not isinstance(feature_variables, list):\n",
        "            feature_variables = [feature_variables]  # Make it a list if it's not\n",
        "\n",
        "        # Filter the dataframe to create a training set\n",
        "        training_data = data.dropna(subset=feature_variables + [target_variable])\n",
        "        training_data = training_data.loc[\n",
        "            (training_data[target_variable].notnull()) &\n",
        "            (training_data[feature_variables[0]] <= max_pop_threshold),\n",
        "            feature_variables + [target_variable]\n",
        "        ]\n",
        "\n",
        "        X_train = training_data[feature_variables]\n",
        "        y_train = training_data[target_variable]\n",
        "\n",
        "        # Fit the KNN model on the training data\n",
        "        self.knn_regressor.fit(X_train, y_train)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def impute_missing_values(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Identify the subset with missing target variable values\n",
        "        missing_data_subset = data.loc[\n",
        "            data[target_variable].isnull() & (data[feature_variables[0]] <= max_pop_threshold),\n",
        "            feature_variables\n",
        "        ]\n",
        "\n",
        "        # Predict the target variable for the missing data subset\n",
        "        predicted_values = self.knn_regressor.predict(missing_data_subset)\n",
        "\n",
        "        # Impute the predicted values back into the original dataframe\n",
        "        data.loc[missing_data_subset.index, target_variable] = predicted_values\n",
        "\n",
        "        return data\n",
        "\n",
        "    def evaluate_model(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Prepare the dataset for cross-validation\n",
        "        valid_data = data.loc[data[target_variable].notnull() & (data[feature_variables[0]] <= max_pop_threshold)]\n",
        "        X = valid_data[feature_variables]\n",
        "        y = valid_data[target_variable]\n",
        "\n",
        "        # Define a custom scorer function\n",
        "        def mae_scorer(estimator, X, y):\n",
        "            predicted_values = estimator.predict(X)\n",
        "            return -np.mean(np.abs(predicted_values - y))  # Negative MAE\n",
        "\n",
        "        # Perform 5-Fold cross-validation to estimate the MAE\n",
        "        cv_scores = cross_val_score(self.knn_regressor, X, y, cv=5, scoring=mae_scorer)\n",
        "\n",
        "        # Return the average MAE\n",
        "        return -cv_scores.mean()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == '__main__':\n",
        "    data = pd.DataFrame(sc)  # Your DataFrame here\n",
        "    target_variable = 'ec_county'  # The variable you wish to impute\n",
        "    feature_variables = ['volunteering_rate_county']\n",
        "    max_pop_threshold = 100000  # Example threshold value\n",
        "\n",
        "    imputer = NumericImputer(n_neighbors=5)\n",
        "    imputer.train_model(data, target_variable, feature_variables, max_pop_threshold)\n",
        "    imputed_data = imputer.impute_missing_values(data, target_variable, feature_variables, max_pop_threshold)\n",
        "    mae_estimate = imputer.evaluate_model(data, target_variable, feature_variables, max_pop_threshold)\n",
        "\n",
        "    print('MAE Estimate:', mae_estimate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9k6dPczBUieB",
        "outputId": "0c6ee37d-d779-4dfb-d4e9-ff503370a2c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE Estimate: 0.12899990750009158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ec_grp_mem_county imputation\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class NumericImputer:\n",
        "    def __init__(self, n_neighbors=5):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.knn_regressor = KNeighborsRegressor(n_neighbors=self.n_neighbors)\n",
        "\n",
        "    def train_model(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Ensure feature_variables is a list for consistent handling\n",
        "        if not isinstance(feature_variables, list):\n",
        "            feature_variables = [feature_variables]  # Make it a list if it's not\n",
        "\n",
        "        # Filter the dataframe to create a training set\n",
        "        training_data = data.dropna(subset=feature_variables + [target_variable])\n",
        "        training_data = training_data.loc[\n",
        "            (training_data[target_variable].notnull()) &\n",
        "            (training_data[feature_variables[0]] <= max_pop_threshold),\n",
        "            feature_variables + [target_variable]\n",
        "        ]\n",
        "\n",
        "        X_train = training_data[feature_variables]\n",
        "        y_train = training_data[target_variable]\n",
        "\n",
        "        # Fit the KNN model on the training data\n",
        "        self.knn_regressor.fit(X_train, y_train)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def impute_missing_values(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Identify the subset with missing target variable values\n",
        "        missing_data_subset = data.loc[\n",
        "            data[target_variable].isnull() & (data[feature_variables[0]] <= max_pop_threshold),\n",
        "            feature_variables\n",
        "        ]\n",
        "\n",
        "        # Predict the target variable for the missing data subset\n",
        "        predicted_values = self.knn_regressor.predict(missing_data_subset)\n",
        "\n",
        "        # Impute the predicted values back into the original dataframe\n",
        "        data.loc[missing_data_subset.index, target_variable] = predicted_values\n",
        "\n",
        "        return data\n",
        "\n",
        "    def evaluate_model(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Prepare the dataset for cross-validation\n",
        "        valid_data = data.loc[data[target_variable].notnull() & (data[feature_variables[0]] <= max_pop_threshold)]\n",
        "        X = valid_data[feature_variables]\n",
        "        y = valid_data[target_variable]\n",
        "\n",
        "        # Define a custom scorer function\n",
        "        def mae_scorer(estimator, X, y):\n",
        "            predicted_values = estimator.predict(X)\n",
        "            return -np.mean(np.abs(predicted_values - y))  # Negative MAE\n",
        "\n",
        "        # Perform 5-Fold cross-validation to estimate the MAE\n",
        "        cv_scores = cross_val_score(self.knn_regressor, X, y, cv=5, scoring=mae_scorer)\n",
        "\n",
        "        # Return the average MAE\n",
        "        return -cv_scores.mean()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == '__main__':\n",
        "    data = pd.DataFrame(sc)  # Your DataFrame here\n",
        "    target_variable = 'ec_grp_mem_county'  # The variable you wish to impute\n",
        "    feature_variables = ['volunteering_rate_county']\n",
        "    max_pop_threshold = 100000  # Example threshold value\n",
        "\n",
        "    imputer = NumericImputer(n_neighbors=5)\n",
        "    imputer.train_model(data, target_variable, feature_variables, max_pop_threshold)\n",
        "    imputed_data = imputer.impute_missing_values(data, target_variable, feature_variables, max_pop_threshold)\n",
        "    mae_estimate = imputer.evaluate_model(data, target_variable, feature_variables, max_pop_threshold)\n",
        "\n",
        "    print('MAE Estimate:', mae_estimate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvRGLmnjWJbS",
        "outputId": "4d11e612-e4a3-4b7f-e847-18520a740749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE Estimate: 0.1612528275648899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ec_high_county imputation\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class NumericImputer:\n",
        "    def __init__(self, n_neighbors=5):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.knn_regressor = KNeighborsRegressor(n_neighbors=self.n_neighbors)\n",
        "\n",
        "    def train_model(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Ensure feature_variables is a list for consistent handling\n",
        "        if not isinstance(feature_variables, list):\n",
        "            feature_variables = [feature_variables]  # Make it a list if it's not\n",
        "\n",
        "        # Filter the dataframe to create a training set\n",
        "        training_data = data.dropna(subset=feature_variables + [target_variable])\n",
        "        training_data = training_data.loc[\n",
        "            (training_data[target_variable].notnull()) &\n",
        "            (training_data[feature_variables[0]] <= max_pop_threshold),\n",
        "            feature_variables + [target_variable]\n",
        "        ]\n",
        "\n",
        "        X_train = training_data[feature_variables]\n",
        "        y_train = training_data[target_variable]\n",
        "\n",
        "        # Fit the KNN model on the training data\n",
        "        self.knn_regressor.fit(X_train, y_train)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def impute_missing_values(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Identify the subset with missing target variable values\n",
        "        missing_data_subset = data.loc[\n",
        "            data[target_variable].isnull() & (data[feature_variables[0]] <= max_pop_threshold),\n",
        "            feature_variables\n",
        "        ]\n",
        "\n",
        "        # Predict the target variable for the missing data subset\n",
        "        predicted_values = self.knn_regressor.predict(missing_data_subset)\n",
        "\n",
        "        # Impute the predicted values back into the original dataframe\n",
        "        data.loc[missing_data_subset.index, target_variable] = predicted_values\n",
        "\n",
        "        return data\n",
        "\n",
        "    def evaluate_model(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Prepare the dataset for cross-validation\n",
        "        valid_data = data.loc[data[target_variable].notnull() & (data[feature_variables[0]] <= max_pop_threshold)]\n",
        "        X = valid_data[feature_variables]\n",
        "        y = valid_data[target_variable]\n",
        "\n",
        "        # Define a custom scorer function\n",
        "        def mae_scorer(estimator, X, y):\n",
        "            predicted_values = estimator.predict(X)\n",
        "            return -np.mean(np.abs(predicted_values - y))  # Negative MAE\n",
        "\n",
        "        # Perform 5-Fold cross-validation to estimate the MAE\n",
        "        cv_scores = cross_val_score(self.knn_regressor, X, y, cv=5, scoring=mae_scorer)\n",
        "\n",
        "        # Return the average MAE\n",
        "        return -cv_scores.mean()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == '__main__':\n",
        "    data = pd.DataFrame(sc)  # Your DataFrame here\n",
        "    target_variable = 'ec_high_county'  # The variable you wish to impute\n",
        "    feature_variables = ['civic_organizations_county']\n",
        "    max_pop_threshold = 100000  # Example threshold value\n",
        "\n",
        "    imputer = NumericImputer(n_neighbors=5)\n",
        "    imputer.train_model(data, target_variable, feature_variables, max_pop_threshold)\n",
        "    imputed_data = imputer.impute_missing_values(data, target_variable, feature_variables, max_pop_threshold)\n",
        "    mae_estimate = imputer.evaluate_model(data, target_variable, feature_variables, max_pop_threshold)\n",
        "\n",
        "    print('MAE Estimate:', mae_estimate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUKxrBd7WKfE",
        "outputId": "fbc38bea-f4e0-4bdb-ba7d-6c1b20b80487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE Estimate: 0.14676499566871226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exposure_grp_mem_county imputation\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class NumericImputer:\n",
        "    def __init__(self, n_neighbors=5):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.knn_regressor = KNeighborsRegressor(n_neighbors=self.n_neighbors)\n",
        "\n",
        "    def train_model(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Ensure feature_variables is a list for consistent handling\n",
        "        if not isinstance(feature_variables, list):\n",
        "            feature_variables = [feature_variables]  # Make it a list if it's not\n",
        "\n",
        "        # Filter the dataframe to create a training set\n",
        "        training_data = data.dropna(subset=feature_variables + [target_variable])\n",
        "        training_data = training_data.loc[\n",
        "            (training_data[target_variable].notnull()) &\n",
        "            (training_data[feature_variables[0]] <= max_pop_threshold),\n",
        "            feature_variables + [target_variable]\n",
        "        ]\n",
        "\n",
        "        X_train = training_data[feature_variables]\n",
        "        y_train = training_data[target_variable]\n",
        "\n",
        "        # Fit the KNN model on the training data\n",
        "        self.knn_regressor.fit(X_train, y_train)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def impute_missing_values(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Identify the subset with missing target variable values\n",
        "        missing_data_subset = data.loc[\n",
        "            data[target_variable].isnull() & (data[feature_variables[0]] <= max_pop_threshold),\n",
        "            feature_variables\n",
        "        ]\n",
        "\n",
        "        # Predict the target variable for the missing data subset\n",
        "        predicted_values = self.knn_regressor.predict(missing_data_subset)\n",
        "\n",
        "        # Impute the predicted values back into the original dataframe\n",
        "        data.loc[missing_data_subset.index, target_variable] = predicted_values\n",
        "\n",
        "        return data\n",
        "\n",
        "    def evaluate_model(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Prepare the dataset for cross-validation\n",
        "        valid_data = data.loc[data[target_variable].notnull() & (data[feature_variables[0]] <= max_pop_threshold)]\n",
        "        X = valid_data[feature_variables]\n",
        "        y = valid_data[target_variable]\n",
        "\n",
        "        # Define a custom scorer function\n",
        "        def mae_scorer(estimator, X, y):\n",
        "            predicted_values = estimator.predict(X)\n",
        "            return -np.mean(np.abs(predicted_values - y))  # Negative MAE\n",
        "\n",
        "        # Perform 5-Fold cross-validation to estimate the MAE\n",
        "        cv_scores = cross_val_score(self.knn_regressor, X, y, cv=5, scoring=mae_scorer)\n",
        "\n",
        "        # Return the average MAE\n",
        "        return -cv_scores.mean()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == '__main__':\n",
        "    data = pd.DataFrame(sc)  # Your DataFrame here\n",
        "    target_variable = 'exposure_grp_mem_county'  # The variable you wish to impute\n",
        "    feature_variables = ['civic_organizations_county']\n",
        "    max_pop_threshold = 100000  # Example threshold value\n",
        "\n",
        "    imputer = NumericImputer(n_neighbors=5)\n",
        "    imputer.train_model(data, target_variable, feature_variables, max_pop_threshold)\n",
        "    imputed_data = imputer.impute_missing_values(data, target_variable, feature_variables, max_pop_threshold)\n",
        "    mae_estimate = imputer.evaluate_model(data, target_variable, feature_variables, max_pop_threshold)\n",
        "\n",
        "    print('MAE Estimate:', mae_estimate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MqlsWHTWLRl",
        "outputId": "4f54dc08-a09a-4a15-ee8f-2224d8d12e82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE Estimate: 0.17806420308847212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exposure_grp_mem_high_county imputation\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class NumericImputer:\n",
        "    def __init__(self, n_neighbors=5):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.knn_regressor = KNeighborsRegressor(n_neighbors=self.n_neighbors)\n",
        "\n",
        "    def train_model(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Ensure feature_variables is a list for consistent handling\n",
        "        if not isinstance(feature_variables, list):\n",
        "            feature_variables = [feature_variables]  # Make it a list if it's not\n",
        "\n",
        "        # Filter the dataframe to create a training set\n",
        "        training_data = data.dropna(subset=feature_variables + [target_variable])\n",
        "        training_data = training_data.loc[\n",
        "            (training_data[target_variable].notnull()) &\n",
        "            (training_data[feature_variables[0]] <= max_pop_threshold),\n",
        "            feature_variables + [target_variable]\n",
        "        ]\n",
        "\n",
        "        X_train = training_data[feature_variables]\n",
        "        y_train = training_data[target_variable]\n",
        "\n",
        "        # Fit the KNN model on the training data\n",
        "        self.knn_regressor.fit(X_train, y_train)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def impute_missing_values(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Identify the subset with missing target variable values\n",
        "        missing_data_subset = data.loc[\n",
        "            data[target_variable].isnull() & (data[feature_variables[0]] <= max_pop_threshold),\n",
        "            feature_variables\n",
        "        ]\n",
        "\n",
        "        # Predict the target variable for the missing data subset\n",
        "        predicted_values = self.knn_regressor.predict(missing_data_subset)\n",
        "\n",
        "        # Impute the predicted values back into the original dataframe\n",
        "        data.loc[missing_data_subset.index, target_variable] = predicted_values\n",
        "\n",
        "        return data\n",
        "\n",
        "    def evaluate_model(self, data, target_variable, feature_variables, max_pop_threshold):\n",
        "        # Prepare the dataset for cross-validation\n",
        "        valid_data = data.loc[data[target_variable].notnull() & (data[feature_variables[0]] <= max_pop_threshold)]\n",
        "        X = valid_data[feature_variables]\n",
        "        y = valid_data[target_variable]\n",
        "\n",
        "        # Define a custom scorer function\n",
        "        def mae_scorer(estimator, X, y):\n",
        "            predicted_values = estimator.predict(X)\n",
        "            return -np.mean(np.abs(predicted_values - y))  # Negative MAE\n",
        "\n",
        "        # Perform 5-Fold cross-validation to estimate the MAE\n",
        "        cv_scores = cross_val_score(self.knn_regressor, X, y, cv=5, scoring=mae_scorer)\n",
        "\n",
        "        # Return the average MAE\n",
        "        return -cv_scores.mean()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == '__main__':\n",
        "    data = pd.DataFrame(sc)  # Your DataFrame here\n",
        "    target_variable = 'exposure_grp_mem_high_county'  # The variable you wish to impute\n",
        "    feature_variables = ['support_ratio_county']\n",
        "    max_pop_threshold = 100000  # Example threshold value\n",
        "\n",
        "    imputer = NumericImputer(n_neighbors=5)\n",
        "    imputer.train_model(data, target_variable, feature_variables, max_pop_threshold)\n",
        "    imputed_data = imputer.impute_missing_values(data, target_variable, feature_variables, max_pop_threshold)\n",
        "    mae_estimate = imputer.evaluate_model(data, target_variable, feature_variables, max_pop_threshold)\n",
        "\n",
        "    print('MAE Estimate:', mae_estimate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlZUzU3mWMGk",
        "outputId": "818228fe-7aeb-481f-db2d-2b27faec8c9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE Estimate: 0.15402975096665858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WpClqqNZfjM",
        "outputId": "c5763c5f-098a-4c95-b074-7a170a597a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "num_below_p50                    2\n",
              "pop2018                          0\n",
              "ec_county                        0\n",
              "child_ec_county                  0\n",
              "ec_grp_mem_county                0\n",
              "ec_high_county                   0\n",
              "child_high_ec_county             0\n",
              "ec_grp_mem_high_county          77\n",
              "exposure_grp_mem_county          0\n",
              "exposure_grp_mem_high_county     0\n",
              "child_exposure_county            0\n",
              "child_high_exposure_county       0\n",
              "bias_grp_mem_county             77\n",
              "bias_grp_mem_high_county        77\n",
              "child_bias_county                0\n",
              "child_high_bias_county           0\n",
              "clustering_county                0\n",
              "support_ratio_county             0\n",
              "volunteering_rate_county         0\n",
              "civic_organizations_county       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Development"
      ],
      "metadata": {
        "id": "4i1VnFBbZlCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Partition the data\n",
        "target = sc['child_ec_county']\n",
        "predictors = sc.drop(['child_ec_county'],axis=1)\n",
        "predictors_train, predictors_test, target_train, target_test = train_test_split(predictors, target, test_size=0.3, random_state=0)\n",
        "print(predictors_train.shape, predictors_test.shape, target_train.shape, target_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igmKJcJifut_",
        "outputId": "1d89dc92-f309-4e4f-b82b-4b85b4393efe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2162, 19) (927, 19) (2162,) (927,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Ensure predictors_train does not contain missing or infinite values\n",
        "valid_indices_train = np.isfinite(predictors_train).all(axis=1) & np.isfinite(target_train)\n",
        "predictors_train_clean = predictors_train[valid_indices_train]\n",
        "target_train_clean = target_train[valid_indices_train]\n",
        "\n",
        "# Fit linear regression model with the cleaned training data\n",
        "model1 = linear_model.LinearRegression()\n",
        "model1.fit(predictors_train_clean, target_train_clean)\n",
        "\n",
        "# Remove rows with missing or infinite values from predictors_test\n",
        "valid_indices_test = np.isfinite(predictors_test).all(axis=1)\n",
        "predictors_test_clean = predictors_test[valid_indices_test]\n",
        "target_test_clean = target_test[valid_indices_test]"
      ],
      "metadata": {
        "id": "tKDgjACPaU3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with missing or infinite values\n",
        "valid_indices = np.isfinite(predictors_train).all(axis=1) & np.isfinite(target_train)\n",
        "predictors_train_clean = predictors_train[valid_indices]\n",
        "target_train_clean = target_train[valid_indices]\n",
        "\n",
        "# Show model summary\n",
        "X2 = sm.add_constant(predictors_train_clean)\n",
        "y = target_train_clean\n",
        "est = sm.OLS(y, X2)\n",
        "est2 = est.fit()\n",
        "print(est2.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2psWJKWaX4u",
        "outputId": "14d5fba6-a6ba-4c73-e2a6-10596fecc663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:        child_ec_county   R-squared:                       0.999\n",
            "Model:                            OLS   Adj. R-squared:                  0.999\n",
            "Method:                 Least Squares   F-statistic:                 1.104e+05\n",
            "Date:                Fri, 09 Feb 2024   Prob (F-statistic):               0.00\n",
            "Time:                        06:22:14   Log-Likelihood:                 7526.6\n",
            "No. Observations:                2106   AIC:                        -1.501e+04\n",
            "Df Residuals:                    2086   BIC:                        -1.490e+04\n",
            "Df Model:                          19                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "================================================================================================\n",
            "                                   coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------------------------\n",
            "const                           -0.0878      0.014     -6.136      0.000      -0.116      -0.060\n",
            "num_below_p50                 3.939e-08   1.19e-08      3.310      0.001    1.61e-08    6.27e-08\n",
            "pop2018                      -4.901e-09   1.66e-09     -2.954      0.003   -8.15e-09   -1.65e-09\n",
            "ec_county                       -0.0028      0.006     -0.487      0.626      -0.014       0.008\n",
            "ec_grp_mem_county                0.1471      0.013     11.201      0.000       0.121       0.173\n",
            "ec_high_county                  -0.0031      0.007     -0.457      0.648      -0.016       0.010\n",
            "child_high_ec_county             0.0651      0.016      3.972      0.000       0.033       0.097\n",
            "ec_grp_mem_high_county          -0.0138      0.013     -1.082      0.279      -0.039       0.011\n",
            "exposure_grp_mem_county         -0.1373      0.012    -11.584      0.000      -0.161      -0.114\n",
            "exposure_grp_mem_high_county     0.0194      0.014      1.357      0.175      -0.009       0.047\n",
            "child_exposure_county            0.9992      0.003    298.097      0.000       0.993       1.006\n",
            "child_high_exposure_county      -0.0722      0.018     -4.082      0.000      -0.107      -0.037\n",
            "bias_grp_mem_county              0.1025      0.012      8.732      0.000       0.080       0.126\n",
            "bias_grp_mem_high_county        -0.0240      0.011     -2.137      0.033      -0.046      -0.002\n",
            "child_bias_county               -0.7375      0.005   -139.456      0.000      -0.748      -0.727\n",
            "child_high_bias_county           0.0792      0.012      6.378      0.000       0.055       0.104\n",
            "clustering_county               -0.0092      0.011     -0.870      0.385      -0.030       0.012\n",
            "support_ratio_county             0.0876      0.014      6.068      0.000       0.059       0.116\n",
            "volunteering_rate_county         0.0109      0.005      2.211      0.027       0.001       0.020\n",
            "civic_organizations_county      -0.0063      0.022     -0.287      0.774      -0.050       0.037\n",
            "==============================================================================\n",
            "Omnibus:                      574.548   Durbin-Watson:                   1.933\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             7343.722\n",
            "Skew:                          -0.915   Prob(JB):                         0.00\n",
            "Kurtosis:                      11.963   Cond. No.                     7.30e+07\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The condition number is large, 7.3e+07. This might indicate that there are\n",
            "strong multicollinearity or other numerical problems.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the cleaned testing data\n",
        "prediction_on_test = model1.predict(predictors_test_clean)"
      ],
      "metadata": {
        "id": "pmlV0E4oaaLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the evaluation results on testing data: MAE and RMSE\n",
        "MAE = mean_absolute_error(target_test_clean, prediction_on_test)\n",
        "RMSE = mean_squared_error(target_test_clean, prediction_on_test, squared=False)\n",
        "print(\"MAE:\", MAE)\n",
        "print(\"RMSE:\", RMSE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhqYHklqaeOi",
        "outputId": "421e940d-8a22-4a4b-cc26-0894d5a26ed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 0.004524477213677742\n",
            "RMSE: 0.006964966950267455\n"
          ]
        }
      ]
    }
  ]
}